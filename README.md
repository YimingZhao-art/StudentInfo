# StudentInfo

Q1

要证明一个函数是严格凸函数的两个等价条件是：图像上任意两点间的线段位于这两点间图像的上方，和该函数的二阶导数为正，我们可以从数学定义出发。

首先，定义一个一元实值函数$f(x)$在区间$I$上是严格凸的，如果对于任意$x_1, x_2 \in I$和任意$\lambda \in (0, 1)$，都有：

$$
f(\lambda x_1 + (1 - \lambda)x_2) < \lambda f(x_1) + (1 - \lambda)f(x_2)
$$

这个不等式表明，函数在$x_1$和$x_2$之间的值严格小于直线$x_1$和$x_2$之间的值，这是严格凸性的直观表述。

现在要证明这个条件与函数的二阶导数为正是等价的，我们来考虑函数的泰勒展开。对于函数$f(x)$，如果它在$I$上二次可导，那么对于$x \in I$，存在$\xi$介于$x_1$和$x_2$之间，使得$f(x)$可以表示为：

$$
f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{2}f''(\xi)(x - x_0)^2
$$

其中$x_0$是$x_1$和$x_2$之间的某个点，这里可以取$x_0 = \lambda x_1 + (1 - \lambda)x_2$。如果$f''(x) > 0$对于所有$x \in I$成立，那么上面的泰勒展开中的二阶项是正的。这意味着$f(x)$在点$x_0$处的切线以下，也就是说，$f(x)$的图像位于直线$x_1$到$x_2$之间的线段上方。

反过来，如果二阶导数$f''(x)$在某点不是正的，则在这一点附近的函数图像不会严格位于连接两侧点的直线上方。因此，如果函数是严格凸的，则其二阶导数应该在定义域内的任意点上都是正的。

综上所述，如果一个函数$f(x)$的二阶导数$f''(x)>0$，那么它是严格凸的；如果一个函数是严格凸的，那么它的二阶导数$f''(x)>0$。这两个条件是等价的。


## Q2

确定k-means算法中最优的k值常用的方法是肘部法则（Elbow Method）。具体步骤如下：

1. 确定k值的取值范围：选择一个最小的k值开始，比如k=1，然后逐步增大k值，比如最大到k=10或者更大，取决于数据集的大小和特性。

2. 对每个k值运行k-means算法：对每一个k值，都使用k-means算法对数据集进行聚类，并计算每次聚类的成本函数，通常是各个点到其最近的聚类中心的距离平方和，即SSE（Sum of Squared Errors）。

$$SSE(k) = \sum_{i=1}^{n} \min_{\mu_j \in C}(||x_i - \mu_j||^2)$$ 

其中，$n$是数据点的数量，$C$是选定的k个聚类中心，$x_i$是第i个数据点，$\mu_j$是最接近$x_i$的聚类中心。

3. 画出成本函数的图：将每个k值对应的成本函数值画在图上，横坐标是k值，纵坐标是对应的SSE值。

4. 分析肘部点：观察图形，随着k值的增加，SSE会下降，因为聚类中心越多，数据点到聚类中心的距离自然越小。但在某个点后，增加k将不会显著降低SSE，即SSE的下降速度会减缓，这个点类似于人的肘部，因此被称为肘部点。选择这个点对应的k值作为最优的k值。

5. 选择最优的k值：最优的k值就是肘部点对应的k值，因为在这个点之后增加更多的聚类中心并不会显著提高聚类的效果，而只会增加计算复杂度和资源消耗。

使用肘部法则时，有时可能不会得到一个明显的肘部点，这时可以结合业务知识或其他评估指标（如轮廓系数）来帮助确定最优的k值。

## Q3

在朴素贝叶斯分类器中，当训练数据中某个类别下某个属性的值没有出现时，会导致该属性值的概率为零。这会使得整个数据样本的后验概率也为零，这显然不合理。为了解决这个问题，可以采用平滑技术，比如拉普拉斯平滑（Laplace smoothing）和m-估计（m-estimate）。

1. 拉普拉斯平滑（Laplace Smoothing）:
拉普拉斯平滑也被称为加1平滑，因为它将所有可能的属性值的计数都加1，以避免出现零概率。具体来说，对于某个属性值的计数，我们使用以下公式来估计条件概率：
$$P(x_i|y) = \frac{N_{x_i, y} + 1}{N_y + d}$$
其中，$N_{x_i, y}$是在类别$y$下属性值$x_i$出现的次数，$N_y$是类别$y$下所有样本的总数，$d$是该属性的可能取值数量。

2. m-估计（m-Estimate）:
m-估计是对拉普拉斯平滑的一种泛化。在m-估计中，我们引入一个先验概率$p$和一个权重$m$，这样可以在估计概率时平衡先验信息和样本信息。m-估计的公式为：
$$P(x_i|y) = \frac{N_{x_i, y} + m \cdot p}{N_y + m}$$
其中$m$是一个正的常数，$p$通常是根据先验知识选择的属性值$x_i$在类别$y$下的概率。

这两种平滑方法通过防止零概率的出现，使得朴素贝叶斯分类器能够更好地处理未在训练数据中观察到的特征组合。这不仅提高了模型的鲁棒性，也因此可能提高了模型的准确度。此外，它们也有助于防止过拟合，因为它们引入了一定程度的正则化。

## Q4

梯度下降法(Gradient Descent)和正规方程法(Normal Equations)是求解线性回归问题的两种常用方法，它们各自有优缺点。

梯度下降法的优点：
1. 适用性强：对于特征数量很多的情况，梯度下降法依然适用。
2. 扩展性好：可用于各种可微分的损失函数。
3. 内存占用少：每次迭代只需要处理一批数据，而不需要加载整个数据集。

梯度下降法的缺点：
1. 需要选择合适的学习率。
2. 需要多次迭代，可能会收敛较慢。
3. 可能会陷入局部最小值（在线性回归的凸优化问题中，这不是问题）。

正规方程法的优点：
1. 不需要选择学习率。
2. 不需要迭代，一次计算得到最优解。
3. 在特征数量不多时，计算速度快。

正规方程法的缺点：
1. 当特征数量很多时，计算$(X^TX)^{-1}$会非常慢，并且数值稳定性差。
2. 内存占用大，需要将整个数据集加载到内存中。
3. 不适用于特征数量比样本数量还多的情况（即$X^TX$不可逆或者求逆非常困难）。

选择使用哪种方法取决于数据集的特点和计算资源的限制。如果特征数量不多（例如数百个以内），可以使用正规方程法快速得到解。如果特征数量非常多或者数据集太大无法一次性加载到内存中，那么应该使用梯度下降法。此外，如果要处理的是在线学习或者实时更新模型的问题，梯度下降法也是更好的选择。

## Q5

在图挖掘中，链接预测和节点分类是两种不同的任务。

链接预测(link prediction)的目的是预测图中不同节点间是否可能存在链接。换句话说，给定一个图，我们希望预测图中哪些节点对之间会形成一个新的边。这在社交网络分析、推荐系统等领域有广泛应用。

节点分类(node classification)是指预测图中单个节点属于哪个类别的任务。这里的分类标准可以基于节点的属性、它们在图中的位置或它们的邻域结构。节点分类可以应用于社群发现、蛋白质功能分类等场景。

设计一个链接预测系统通常包括以下几个步骤：

1. **构建链接预测数据集**：
   - 选择一个现有的图或采集新的图数据。
   - 移除图中的一部分边以形成一个训练集（已知链接）和测试集（未知链接）。
   - 确保测试集中的边在训练集中不存在，但可能包括训练集中未出现的节点。

2. **选择特征**：
   - **节点级特征**：如节点的度、聚类系数等。
   - **边级特征**：如两个节点之间的最短路径、共同邻居的数量、Jaccard系数等。
   - **图级特征**：如图的密度、直径等。
   - **基于嵌入的特征**：如通过node2vec、GraphSAGE等图嵌入算法得到的节点向量表示。

3. **选择模型**：
   - 可以使用传统的机器学习模型，如逻辑回归、支持向量机(SVM)、随机森林等。
   - 也可以使用基于图的神经网络模型，如图卷积网络(GCN)、图注意力网络(GAT)等。

4. **评估性能**：
   - 使用常见的分类指标，如准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1分数等。
   - 也可以使用排名指标，如平均精度均值(Mean Average Precision, MAP)、接收者操作特征曲线下的面积(AUC)等。

在实际操作中，还需要对数据集进行预处理，如特征标准化，对模型进行调参，以及使用交叉验证等方法来避免过拟合，并保证模型的泛化能力。

## Q6

精确度（Precision）和召回率（Recall）是二分类模型性能评估中的两个重要指标。

精确度是指模型预测为正类别中实际为正类别的比例，计算公式为：
$$Precision = \frac{TP}{TP + FP}$$
其中，$TP$ 表示真正例（True Positives）的数量，即正确预测为正类的数量；$FP$ 表示假正例（False Positives）的数量，即错误预测为正类的数量。

召回率是指实际为正类别中模型预测为正类别的比例，计算公式为：
$$Recall = \frac{TP}{TP + FN}$$
其中，$FN$ 表示假负例（False Negatives）的数量，即错误预测为负类的数量。

高精确度比高召回率更重要的场景举例：在金融欺诈检测中，可能更希望模型预测出来的欺诈案例都是真的，即偏好高精确度，因为错误地标记正常交易为欺诈可能导致客户不满和资源浪费。

高召回率比高精确度更重要的场景举例：在疾病筛查中，可能更希望覆盖所有可能的病例，即偏好高召回率，因为漏掉任何一个真正的病例可能导致严重的健康后果。

## Q7

逻辑回归模型在本质上是一个二分类模型，但是可以通过一些策略来进行多类别分类，常见的有一对多（One-vs-Rest, OvR）和一对一（One-vs-One, OvO）方法。

一对多（OvR）策略：
1. 对于每一个类别$i$，将该类别的样本作为正类，其他所有类别的样本作为负类。
2. 训练多个二分类的逻辑回归模型，每个模型对应一个类别。
3. 在预测时，对于一个新的输入样本，分别使用这些模型进行预测，选择概率最高的类别作为最终的分类结果。

一对一（OvO）策略：
1. 对于$K$个类别的分类问题，构建$K(K-1)/2$个二分类器，每个分类器负责将一对类别区分开。
2. 在预测时，新输入样本将被所有的二分类器预测一遍，采用投票的方式，即每个分类器投票给它预测的类别，最终得票最多的类别作为预测结果。

在实际应用中，一对多策略由于模型数量较少，计算复杂度低，因此更常用。而对于类别之间区分度不高的问题，一对一策略可能会有更好的效果，但计算复杂度较高。

逻辑回归模型的训练通常使用极大似然估计方法，通过迭代算法如梯度下降来求解模型参数。在多分类的情况下，逻辑回归模型的目标函数会相应修改，以适应多个类别的情况。对于一对多策略，还可以使用softmax函数将线性输出转换为概率分布，这通常被称为多项逻辑回归或softmax回归。

## Q8

MapReduce框架可以用于在分布式环境中计算PageRank，其工作过程大致如下：

1. **Map阶段**：在这个阶段，Map函数处理输入数据，通常是网页集合及其链接信息。对于每一个网页，Map函数会输出一系列键值对。键是链接到的网页ID，值是当前网页的PageRank值除以其出链数，表示通过这条链接传递的PageRank值。此外，Map函数还会输出当前页面及其所有出链，以便在后续迭代中使用。

   具体的Map操作如下：
   - 输入：每个网页的PageRank值及其出链列表。
   - 处理：对于每个网页，Map函数计算分配给其每个出链的PageRank份额（当前页面的PageRank除以其出链数目）。
   - 输出：键值对，键是目标网页ID，值是传递的PageRank份额；以及当前页面的出链列表。

2. **Shuffle阶段**：MapReduce框架的Shuffle阶段会自动将Map输出的键值对进行排序和分组，使得相同键的值集合在一起传递给Reduce函数。

3. **Reduce阶段**：在Reduce阶段，每个Reduce函数接收到某个目标网页ID及其对应的所有PageRank份额的列表。Reduce函数将这些份额相加，得到目标网页的新的PageRank值。然后，为了完成PageRank算法，可能还需要加上阻尼因子，典型值为0.85，表示85%的PageRank值来自传递，余下15%平均分配给所有网页，确保系统是闭环的。

   具体的Reduce操作如下：
   - 输入：目标网页ID以及该网页收到的所有PageRank份额列表。
   - 处理：Reduce函数将所有份额相加，并乘以阻尼因子，然后加上随机浏览的概率。
   - 输出：目标网页的新PageRank值。

这个过程会迭代执行多次，直到所有网页的PageRank值收敛。在每次迭代之后，新计算出的PageRank值会被用作下一次迭代的输入值。这种方式使得PageRank算法可以在分布式系统中高效地处理大规模数据集。

## Q9

PCA（主成分分析）用于图像压缩的过程包括以下步骤：

1. 数据标准化：将图像数据进行标准化处理，使其均值为0，方差为1。对于图像来说，通常涉及将每个像素值减去平均像素值，然后除以标准差。

2. 计算协方差矩阵：构造协方差矩阵来表示图像数据的变异性。如果将图像数据矩阵表示为$X$，其中每行是一个展平的图像，协方差矩阵$C$可以通过以下公式计算：
   $$C = \frac{1}{n-1} X^T X$$
   其中，$n$是图像样本的数量。

3. 计算特征值和特征向量：对协方差矩阵$C$进行特征分解，得到特征值和对应的特征向量。这些特征向量就是主成分。

4. 选择主成分：根据特征值的大小选择最重要的$k$个特征向量，这些特征向量代表了数据中的主要变异性。通常会根据特征值累计贡献率来选择$k$。

5. 转换到新空间：使用选定的$k$个特征向量构建投影矩阵$W$，然后将原始数据$X$投影到这些主成分上，得到压缩后的表示$Y$：
   $$Y = XW$$

6. 图像重构：使用投影矩阵$W$和压缩后的表示$Y$重构图像。如果用$W'$表示$W$中所选特征向量的逆矩阵，重构后的图像$X'$可以通过以下公式得到：
   $$X' = YW'$$

7. 逆标准化：将重构后的图像数据逆标准化，恢复到原始的像素值范围。

通过这个过程，原始图像通过PCA转换到一个低维空间，然后再从这个低维空间重构回来，实现图像的压缩和重构。需要注意的是，选择较少的主成分会增加压缩率，但同时也会降低重构图像的质量。

## Q10

用户基于（User-based）和物品基于（Item-based）推荐系统是协同过滤（Collaborative Filtering）方法的两种主要类型。

用户基于推荐系统的核心思想是找到相似的用户，并基于这些用户的历史喜好来为目标用户推荐物品。它的主要步骤包括：

1. 计算用户之间的相似度。
2. 找到与目标用户最相似的K个用户（K近邻）。
3. 综合这些相似用户的喜好，对目标用户可能感兴趣的物品进行预测和推荐。

物品基于推荐系统的核心思想是找到物品之间的相似性，并基于用户历史上对相似物品的喜好来进行推荐。它的主要步骤包括：

1. 计算物品之间的相似度。
2. 对于目标用户已经喜欢的每个物品，找到与之最相似的物品。
3. 根据物品之间的相似度和用户的历史喜好，为用户推荐可能感兴趣的其他物品。

用户基于推荐系统的优点在于能够利用用户群体的智慧，它假设相似的用户可能会喜欢相似的物品。但是它的缺点包括处理新用户困难（冷启动问题），并且随着用户数量的增加，计算复杂度会急剧上升。

相比之下，物品基于推荐系统通常计算效率更高，因为物品的总数往往比用户数小，且物品之间的相似度不会随着时间变化太大。此外，物品基于方法对新用户更友好，因为可以根据用户的少量行为就进行有效的推荐。但是，它可能不如用户基于推荐系统那样能够捕捉到复杂的用户偏好。

## Q11

使用聚类分析来识别可能错误标记的文本数据集中的样本，可以遵循以下步骤：

1. **文本向量化**：首先需要将文本数据转换为数值向量，这一步通常通过词袋模型（Bag of Words）、TF-IDF（Term Frequency-Inverse Document Frequency）或者词嵌入（如Word2Vec、GloVe或BERT embeddings）来实现。

2. **选择合适的聚类算法**：根据数据集的特点选择一个合适的聚类算法。常见的算法包括K-Means、谱聚类（Spectral Clustering）、DBSCAN等。参数选择（如K-Means中的k值）对结果影响很大，可能需要尝试不同的参数以找到最佳配置。

3. **执行聚类**：在向量化的数据上应用聚类算法，将数据分成若干个群组。每个群组内的样本在特征空间中彼此相似。

4. **分析聚类结果**：检视聚类结果，特别是那些在特征空间内与其他同标签样本距离较远的点。例如，在情感分析任务中，将聚类得到的群组与情绪标签（正面或负面）进行对比，如果一个群组主要由正面情绪的样本组成，但其中包含了少数负面情绪的样本，这些负面情绪的样本可能就是标记错误的。

5. **识别疑似错误标签**：将聚类后与大多数同类标签样本不在同一群组中的样本标记为疑似错误标签。这些样本可能就是我们要找的标签错误的样本。

6. **进一步验证**：对于这些疑似错误标签的样本，可以采取更深入的分析，比如检查它们的文本内容，或者使用模型预测的置信度进行辅助判断。

需要注意的是，聚类分析本身是无监督的，不涉及标签信息，所以它不直接告诉我们哪些样本标记错误，而是通过样本在特征空间中的分布来间接指出可能的问题。在实际操作中，聚类分析可以与其他方法（如模型置信度筛选等）结合使用，以提高识别错误标签样本的准确性。
